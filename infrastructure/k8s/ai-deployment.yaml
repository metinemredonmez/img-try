apiVersion: apps/v1
kind: Deployment
metadata:
  name: vidcv-ai
  namespace: vidcv
  labels:
    app: vidcv
    component: ai-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vidcv
      component: ai-service
  template:
    metadata:
      labels:
        app: vidcv
        component: ai-service
    spec:
      containers:
        - name: ai-service
          image: ghcr.io/metinemredonmez/img-try/ai-service:latest
          ports:
            - containerPort: 8001
          env:
            - name: LLM_PROVIDER
              value: "ollama"
            - name: OLLAMA_HOST
              value: "http://ollama-service:11434"
            - name: VECTOR_DB
              value: "pgvector"
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: vidcv-secrets
                  key: database-url
            - name: REDIS_URL
              valueFrom:
                secretKeyRef:
                  name: vidcv-secrets
                  key: redis-url
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "2000m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8001
            initialDelaySeconds: 60
            periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: vidcv-ai
  namespace: vidcv
spec:
  selector:
    app: vidcv
    component: ai-service
  ports:
    - port: 8001
      targetPort: 8001
  type: ClusterIP
---
# Ollama Deployment (Local LLM)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: vidcv
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          resources:
            requests:
              memory: "4Gi"
              cpu: "2000m"
            limits:
              memory: "16Gi"
              cpu: "8000m"
              nvidia.com/gpu: 1  # GPU support
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
  namespace: vidcv
spec:
  selector:
    app: ollama
  ports:
    - port: 11434
      targetPort: 11434
  type: ClusterIP
